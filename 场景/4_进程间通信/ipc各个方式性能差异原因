IPC（进程间通信）方式的性能差异，主要由数据传递路径、内核参与程度、同步机制和数据拷贝次数等因素决定：

管道（pipe）/消息队列

数据需要从一个进程用户空间拷贝到内核缓冲区，再从内核缓冲区拷贝到另一个进程用户空间（两次拷贝）。
内核需要管理队列、同步、阻塞/唤醒，开销较大。
适合小数据量、结构化消息，延迟和吞吐量一般。
共享内存（shared memory）

多个进程直接访问同一块物理内存，无需数据拷贝，只需一次映射。
内核只负责建立映射，数据交换完全在用户空间完成，延迟极低、吞吐量极高。
需要额外同步机制（如信号量），适合大数据量、高实时性场景。
套接字（socket）

数据通过内核协议栈传递，通常也涉及多次拷贝。
支持网络通信，灵活但本地性能不如共享内存。
信号/文件/其他

主要用于事件通知或小量数据传递，性能受限于内核调度和文件系统。
总结：

共享内存最快，因为数据不需要在进程间拷贝，适合高性能场景。
管道、消息队列、套接字等方式都需要内核中转和多次拷贝，性能相对较低，但使用简单、同步方便。
选择哪种IPC方式，要根据数据量、实时性、易用性和安全性等需求权衡。
