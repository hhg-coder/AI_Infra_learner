资源：https://paddlepedia.readthedocs.io/en/latest/index.html

一、卷积
https://www.zhihu.com/question/22298352/answer/228543288
https://www.zhihu.com/question/22298352

深度学习卷积核的计算：
比如，3通道卷积核，图像是3通道的，那么有多少个卷积核就输出多少通道
以彩色图像为例，包含三个通道，分别表示RGB三原色的像素值，输入为（3,5,5），分别表示3个通道，每个通道的宽为5，高为5。
假设卷积核只有1个，卷积核通道为3，每个通道的卷积核大小仍为3x3，padding=0，stride=1。
卷积过程如下，每一个通道的像素值与对应的卷积核通道的数值进行卷积，因此每一个通道会对应一个输出卷积结果，三个卷积结果对应位置累加求和，
得到最终的卷积结果（这里卷积输出结果通道只有1个，因为卷积核只有1个。卷积多输出通道下面会继续讲到）。
可以这么理解：最终得到的卷积结果是原始图像各个通道上的综合信息结果。

公式：H_out = (H + 2*Padding - Kernel)/ Stride + 1;

感受野：卷积所得结果中，每个特征图像素点取值依赖于输入图像中的某个区域，该区域被称为感受野（receptive field），
正所谓“管中窥豹、见微知著”。那么这个区域在哪呢，在卷积神经网络中，感受野是特征图（feature map）上的点对应输入图像上的区域。感受野内每个元素数值的变动，都会影响输出点的数值变化。
比如3×3卷积对应的感受野大小就是3×3，而当通过两层3×3的卷积之后，感受野的大小将会增加到5×5
因此，当增加卷积网络深度的同时，感受野将会增大，输出特征图中的一个像素点将会包含更多的图像语义信息。


优化策略之AdamW：
L2 正则化是减少过拟合的经典方法，它会向损失函数添加由模型所有权重的平方和组成的惩罚项，并乘上特定的超参数以控制惩罚力度。
1. 核心思想：惩罚大权重，鼓励简单模型
过拟合的本质是模型过于复杂，​​“记忆”​​了训练数据中的噪声和细节，而不是学习其内在的普遍规律。一个复杂的模型通常表现为其权重（参数）的值非常大且分布散乱。
L2正则化通过直接对模型的权重大小进行惩罚来解决这个问题。它背后的哲学是奥卡姆剃刀原理：​​在同样能解释数据的模型中，越简单的模型越可能泛化得好​​。
工作原理：修改损失函数
L2正则化通过修改模型优化的目标函数（损失函数）来实现这一目的。
​​原始损失函数​​： 我们原本的目标是最小化损失函数，例如均方误差（MSE）或交叉熵（Cross-Entropy）。这可以表示为：
Minimize: Loss(weights; data)
​​加入L2正则化后的损失函数​​： L2正则化在原始损失函数的基础上，加上了所有权重平方和（乘以一个系数）作为惩罚项。
Minimize: Loss(weights; data) + λ * Σ(weights²)
Σ(weights²)： 这是L2范数的平方，计算了所有权重值的平方和。这个值越大，说明模型的权重越大、越复杂，我们越要惩罚它。
λ（Lambda）： 这是一个​​超参数​​，称为​​正则化强度​​。它控制我们对大权重的惩罚力度：
λ = 0： 正则项失效，模型退回到原始状态。
λ很小： 惩罚力度小，模型可能依然复杂。
λ很大： 惩罚力度非常大，模型会极力缩小所有权重，可能导致​​欠拟合​​（模型过于简单，无法捕捉数据中的规律）。


(额外补充L1正则化)：
L1正则化不仅收缩权重，还会将一些不重要的特征的权重​​精确地设置为0​​。得到的是一个​​稀疏​​模型，相当于进行了​​特征选择​​，剔除了不重要的特征。
从​​几何直观​​和​​数学优化​​两个角度来理解。
1. 几何直观：约束区域的“尖角”
我们想象一个最简单的场景：模型只有两个权重，w1和 w2。
​​我们的目标​​： 最小化原始的损失函数（比如平方误差）Loss(w1, w2)。
​​施加的约束​​： L1正则化要求参数的绝对值之和不能太大，即 |w1| + |w2| <= t，其中 t是一个由正则化强度 λ 决定的常数。
这个约束条件 |w1| + |w2| <= t在几何上是一个​​菱形​​（ diamond ）。
现在，我们将最小化损失函数的过程想象成在 (w1, w2) 坐标平面上寻找损失函数“碗”的最低点（等高线图的中心）。
而L1正则化给我们划定的“可活动区域”就是这个菱形。
​​关键点来了​​：
由于菱形的形状是有​​尖角​​的，损失函数的等高线（椭圆）与这个菱形区域相碰（接触）时，​​最有可能碰到的就是这些尖角上​​。
​​在尖角上​​： 尖角点的坐标有一个特点——其中一个坐标值为0（例如上图中的 w1=0）。这就意味着，最优解落在了 w1=0的位置，模型成功地忽略掉了这个特征。
​​对比L2​​： L2的约束区域 w1² + w2² <= t是一个​​平滑的圆形​​。椭圆等高线与圆形相切的点几乎不可能恰好在坐标轴上，因此两个权重都会是很小但不为零的值。
​​结论​​： L1菱形约束的“尖角”特性，使得最优解更容易出现在坐标轴上，从而导致某些权重恰好为0，产生稀疏性。
2. 数学优化：次梯度与精确零点
从数学优化的角度看，L1正则项在零点处是不可导的（有一个“尖点”），这个特性至关重要。
我们通过梯度下降法来更新权重。损失函数加上L1正则项后的总目标函数为：
L = Loss(weights) + λ * |weights|
我们对权重 w求导（更准确地说，是次梯度）：
∂L/∂w = ∂Loss/∂w + λ * sign(w)
其中 sign(w)是符号函数（w>0时为1，w<0时为-1，w=0时不可导，取值范围为[-1, 1]）。
​​权重更新规则​​（梯度下降）为：
w = w - η * (∂Loss/∂w) - η * λ * sign(w)
其中 η是学习率。
现在，我们分析一下会发生什么：
1.​​当 w 是正数时​​： sign(w) = 1。更新规则中的 - η * λ * 1会​​稳定地​​将 w 向零减小。
2.​​当 w 是负数时​​： sign(w) = -1。更新规则中的 - η * λ * (-1) = + η * λ会​​稳定地​​将 w 向零增大。
3.​​当 w 已经非常接近零时​​： 假设当前权重 w是一个很小的正数，比如 0.0001。梯度项 ∂Loss/∂w的值通常也是一个有限的数（比如 0.5）。而惩罚项 η * λ * 1是一个固定的推力。
​​关键过程​​：
如果这个固定的推力 η * λ大于梯度本身带来的更新量 η * (∂Loss/∂w)，即 ​​λ > |∂Loss/∂w|​​，那么这次更新的结果就会让 w 越过零点变成负数：
w_new = 0.0001 - η*0.5 - η*λ
如果 η*0.5 + η*λ > 0.0001，w_new就会小于0。
接下来，如果 w 变成了一个很小的负数，比如 -0.0001，符号函数变成了 -1，惩罚项又会产生一个正的推力 + η * λ把它推回零点。
最终，权重 w会在零点附近震荡。但​​只要条件 λ > |∂Loss/∂w|成立​​，优化器就会发现​​把 w 保持为 0 才是使总目标函数最小的最稳定状态​​。因为任何偏离零点的移动都会受到一个大小为 η * λ的、指向零点的强力惩罚。
​​反之​​，如果损失函数在某特征上的梯度非常大（|∂Loss/∂w| > λ），说明这个特征非常重要，那么梯度下降的力量就能克服正则化的惩罚力，让该特征的权重保持一个较大的非零值。
总结
L1正则化能产生稀疏模型的原因可归结为：
1.​​几何原因​​： L1的约束区域（菱形）有“尖角”，这些尖角位于坐标轴上，使得最优解更容易让某些权重恰好为零。
2.优化原因​​： L1正则项在零点不可导，其梯度（次梯度）是常数大小的力（±λ）。对于不重要的特征（其对应的损失函数梯度很小），这个常数力足以将权重“推过”并“稳定在”零点，从而实现精确的零值。


Sigmoid函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢；偏置偏移（Bias Shift）是什么？
偏置偏移​​指的是：在神经网络中，由于前一层的输出（即后一层的输入）全部为正值，会导致后一层权重的梯度在反向传播时，​​全部为正值或全部为负值​​。
这会迫使权重的更新方向被“束缚”在特定的象限中，只能以​​锯齿形（Zig-Zag）​​ 的路径向最优解逼近，从而极大地降低收敛效率。
举个例子形象化理解：​​
想象一个损失函数的等高线图，最优解在中心。参数空间有两个维度（w1, w2）。理想的最速下降路径应该是直接朝向中心。
​​情况一（零中心输入）​​： 梯度方向可以指向任意方向（比如东北、西南、西北、东南），因此可以相对直接地朝向中心点更新。
​​情况二（非零中心输入，如Sigmoid）​​： 每次的梯度方向只能指向第一象限（正正）或第三象限（负负）。更新路径会变成低效的锯齿形。
